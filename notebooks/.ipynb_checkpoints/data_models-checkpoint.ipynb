{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quickmodel\n",
    "A simple command project I'm putting together. Goal: User runs the file with a specific search term, and the program returns a pickled ML model that is trained to recognize images of the search term. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras \n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "img_dir = os.listdir('../data/')\n",
    "img_count = len(img_dir)\n",
    "type(img_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's generate our dataset from our existing files. Luckily, ``keras`` has a nice way of implementing this from their Image library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 117 images belonging to 2 classes.\n",
      "Found 50 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "image_generator = tf.keras.preprocessing.image.ImageDataGenerator(\n",
    "    rotation_range=20,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    horizontal_flip=True,\n",
    "    rescale=1./255, \n",
    "    validation_split=0.3\n",
    ")\n",
    "\n",
    "IMG_HEIGHT = 256\n",
    "IMG_WIDTH = 256\n",
    "BATCH_SIZE=20\n",
    "STEPS_PER_EPOCH = np.ceil(img_count/BATCH_SIZE)\n",
    "\n",
    "train_generator = image_generator.flow_from_directory(directory='../data',\n",
    "                                                     target_size=(IMG_HEIGHT, IMG_WIDTH),\n",
    "                                                     classes = ['Eggs', 'NOT-Eggs'], #should come from script parameter\n",
    "                                                     subset='training',\n",
    "                                                     class_mode='binary') \n",
    "validation_generator = image_generator.flow_from_directory(directory='../data',\n",
    "                                                          target_size=(IMG_HEIGHT, IMG_WIDTH),\n",
    "                                                          classes= ['Eggs', 'NOT-Eggs'],\n",
    "                                                          subset='validation',\n",
    "                                                          class_mode='binary')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have our dataset, we need to construct a model and train it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks.callbacks import EarlyStopping\n",
    "\n",
    "model = keras.models.Sequential([\n",
    "    Conv2D(32, 3, padding='same', activation='relu'),\n",
    "    MaxPooling2D(),\n",
    "    Conv2D(64, 3, padding='same', activation='relu'),\n",
    "    MaxPooling2D(),\n",
    "    Conv2D(128, 3, padding='same', activation='relu'),\n",
    "    MaxPooling2D(),\n",
    "    Conv2D(32, 2, padding='same', activation='relu'),\n",
    "    MaxPooling2D(),\n",
    "    Flatten(),\n",
    "    Dense(512, activation='relu'),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "opt = SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "model.compile(optimizer=opt,\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's train and test our model. Keep in mind, the dataset is quite small so accuracy might be low. We will probably need to generate more augmented data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "5/5 [==============================] - 21s 4s/step - loss: 0.6873 - accuracy: 0.4362 - val_loss: 0.7132 - val_accuracy: 0.6000\n",
      "Epoch 2/5\n",
      "5/5 [==============================] - 19s 4s/step - loss: 0.6849 - accuracy: 0.5641 - val_loss: 0.6653 - val_accuracy: 0.7000\n",
      "Epoch 3/5\n",
      "5/5 [==============================] - 17s 3s/step - loss: 0.6532 - accuracy: 0.7360 - val_loss: 0.6774 - val_accuracy: 0.5800\n",
      "Epoch 4/5\n",
      "5/5 [==============================] - 23s 5s/step - loss: 0.6390 - accuracy: 0.6594 - val_loss: 0.6212 - val_accuracy: 0.6200\n",
      "Epoch 5/5\n",
      "5/5 [==============================] - 21s 4s/step - loss: 0.5652 - accuracy: 0.7331 - val_loss: 0.5585 - val_accuracy: 0.6200\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x633159650>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit_generator(\n",
    "    train_generator,\n",
    "    steps_per_epoch = train_generator.samples // BATCH_SIZE,\n",
    "    validation_data = validation_generator, \n",
    "    validation_steps = validation_generator.samples // BATCH_SIZE,\n",
    "    epochs = 5\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': [0.6866598977338547, 0.683893116928587, 0.6468434571356013, 0.6354073942107642, 0.5649176055953007], 'accuracy': [0.4899329, 0.59731543, 0.70289856, 0.6979866, 0.72483224], 'val_loss': [0.7131918668746948, 0.6652959287166595, 0.6773954033851624, 0.6211616098880768, 0.5584909319877625], 'val_accuracy': [0.6, 0.7, 0.58, 0.62, 0.62]}\n"
     ]
    }
   ],
   "source": [
    "print(model.history.history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
