{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quickmodel\n",
    "A simple command project I'm putting together. Goal: User runs the file with a specific search term, and the program returns a pickled ML model that is trained to recognize images of the search term. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras \n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "img_dir = os.listdir('../data/')\n",
    "img_count = len(img_dir)\n",
    "type(img_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's generate our dataset from our existing files. Luckily, ``keras`` has a nice way of implementing this from their Image library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 117 images belonging to 2 classes.\n",
      "Found 50 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "image_generator = tf.keras.preprocessing.image.ImageDataGenerator(\n",
    "    rotation_range=20,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    horizontal_flip=True,\n",
    "    rescale=1./255, \n",
    "    validation_split=0.3\n",
    ")\n",
    "\n",
    "IMG_HEIGHT = 256\n",
    "IMG_WIDTH = 256\n",
    "BATCH_SIZE=20\n",
    "STEPS_PER_EPOCH = np.ceil(img_count/BATCH_SIZE)\n",
    "\n",
    "train_generator = image_generator.flow_from_directory(directory='../data',\n",
    "                                                     target_size=(IMG_HEIGHT, IMG_WIDTH),\n",
    "                                                     classes = ['Eggs', 'NOT-Eggs'], #should come from script parameter\n",
    "                                                     subset='training',\n",
    "                                                     class_mode='binary') \n",
    "validation_generator = image_generator.flow_from_directory(directory='../data',\n",
    "                                                          target_size=(IMG_HEIGHT, IMG_WIDTH),\n",
    "                                                          classes= ['Eggs', 'NOT-Eggs'],\n",
    "                                                          subset='validation',\n",
    "                                                          class_mode='binary')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have our dataset, we need to construct a model and train it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.Sequential([\n",
    "    Conv2D(32, 3, padding='same', activation='relu'),\n",
    "    MaxPooling2D(),\n",
    "    Conv2D(64, 3, padding='same', activation='relu'),\n",
    "    MaxPooling2D(),\n",
    "    Conv2D(64, 3, padding='same', activation='relu'),\n",
    "    MaxPooling2D(),\n",
    "    Conv2D(32, 2, padding='same', activation='relu'),\n",
    "    MaxPooling2D(),\n",
    "    Flatten(),\n",
    "    Dense(256, activation='relu'),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "opt = SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "model.compile(optimizer=opt,\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's train and test our model. Keep in mind, the dataset is quite small so accuracy might be low. We will probably need to generate more augmented data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "5/5 [==============================] - 18s 4s/step - loss: 0.6786 - accuracy: 0.5974 - val_loss: 0.6791 - val_accuracy: 0.6000\n",
      "Epoch 2/30\n",
      "5/5 [==============================] - 17s 3s/step - loss: 0.6731 - accuracy: 0.5807 - val_loss: 0.6468 - val_accuracy: 0.7000\n",
      "Epoch 3/30\n",
      "5/5 [==============================] - 16s 3s/step - loss: 0.6394 - accuracy: 0.6906 - val_loss: 0.6167 - val_accuracy: 0.7200\n",
      "Epoch 4/30\n",
      "5/5 [==============================] - 18s 4s/step - loss: 0.5891 - accuracy: 0.7401 - val_loss: 0.5935 - val_accuracy: 0.6800\n",
      "Epoch 5/30\n",
      "5/5 [==============================] - 19s 4s/step - loss: 0.5503 - accuracy: 0.6411 - val_loss: 0.4779 - val_accuracy: 0.7400\n",
      "Epoch 6/30\n",
      "5/5 [==============================] - 21s 4s/step - loss: 0.4831 - accuracy: 0.7958 - val_loss: 0.4569 - val_accuracy: 0.8600\n",
      "Epoch 7/30\n",
      "5/5 [==============================] - 15s 3s/step - loss: 0.5708 - accuracy: 0.6969 - val_loss: 0.5657 - val_accuracy: 0.7800\n",
      "Epoch 8/30\n",
      "5/5 [==============================] - 17s 3s/step - loss: 0.5358 - accuracy: 0.7747 - val_loss: 0.4740 - val_accuracy: 0.7600\n",
      "Epoch 9/30\n",
      "5/5 [==============================] - 16s 3s/step - loss: 0.4766 - accuracy: 0.7416 - val_loss: 0.4308 - val_accuracy: 0.8400\n",
      "Epoch 10/30\n",
      "5/5 [==============================] - 15s 3s/step - loss: 0.3708 - accuracy: 0.8257 - val_loss: 0.3248 - val_accuracy: 0.8200\n",
      "Epoch 11/30\n",
      "5/5 [==============================] - 20s 4s/step - loss: 0.3977 - accuracy: 0.7999 - val_loss: 0.3176 - val_accuracy: 0.8200\n",
      "Epoch 12/30\n",
      "5/5 [==============================] - 17s 3s/step - loss: 0.2709 - accuracy: 0.8819 - val_loss: 0.2319 - val_accuracy: 0.9200\n",
      "Epoch 13/30\n",
      "5/5 [==============================] - 17s 3s/step - loss: 0.3107 - accuracy: 0.8536 - val_loss: 0.2392 - val_accuracy: 0.9000\n",
      "Epoch 14/30\n",
      "5/5 [==============================] - 16s 3s/step - loss: 0.1713 - accuracy: 0.9311 - val_loss: 0.4336 - val_accuracy: 0.8400\n",
      "Epoch 15/30\n",
      "5/5 [==============================] - 18s 4s/step - loss: 0.2758 - accuracy: 0.8806 - val_loss: 0.2477 - val_accuracy: 0.8800\n",
      "Epoch 16/30\n",
      "5/5 [==============================] - 17s 3s/step - loss: 0.2842 - accuracy: 0.9015 - val_loss: 0.2959 - val_accuracy: 0.8800\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x63a23ca50>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "model.fit_generator(\n",
    "    train_generator,\n",
    "    steps_per_epoch = train_generator.samples // BATCH_SIZE,\n",
    "    validation_data = validation_generator, \n",
    "    validation_steps = validation_generator.samples // BATCH_SIZE,\n",
    "    epochs = 30,\n",
    "    callbacks=[EarlyStopping(monitor='val_loss', patience=4)],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.88\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "print(model.history.history['val_accuracy'][-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our model is performing at about 88% cross-validated accuracy. Definitely not amazing, and there are architectural improvements that I have in mind to work on in the future, but this is far better than the 60% baseline I started out with. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
