{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quickmodel\n",
    "A simple command project I'm putting together. Goal: User runs the file with a specific search term, and the program returns a pickled ML model that is trained to recognize images of the search term. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras \n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "img_dir = os.listdir('../data/')\n",
    "img_count = len(img_dir)\n",
    "type(img_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's generate our dataset from our existing files. Luckily, ``keras`` has a nice way of implementing this from their Image library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 126 images belonging to 2 classes.\n",
      "Found 54 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "image_generator = tf.keras.preprocessing.image.ImageDataGenerator(\n",
    "    rotation_range=20,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    horizontal_flip=True,\n",
    "    rescale=1./255, \n",
    "    validation_split=0.3\n",
    ")\n",
    "\n",
    "IMG_HEIGHT = 256\n",
    "IMG_WIDTH = 256\n",
    "BATCH_SIZE=20\n",
    "STEPS_PER_EPOCH = np.ceil(img_count/BATCH_SIZE)\n",
    "\n",
    "train_generator = image_generator.flow_from_directory(directory='../data',\n",
    "                                                     target_size=(IMG_HEIGHT, IMG_WIDTH),\n",
    "                                                     classes = ['Eggs', 'NOT-Eggs'], #should come from script parameter\n",
    "                                                     subset='training',\n",
    "                                                     class_mode='binary') \n",
    "validation_generator = image_generator.flow_from_directory(directory='../data',\n",
    "                                                          target_size=(IMG_HEIGHT, IMG_WIDTH),\n",
    "                                                          classes= ['Eggs', 'NOT-Eggs'],\n",
    "                                                          subset='validation',\n",
    "                                                          class_mode='binary')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have our dataset, we need to construct a model and train it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.Sequential([\n",
    "    Conv2D(32, 3, padding='same', activation='relu'),\n",
    "    MaxPooling2D(),\n",
    "    Conv2D(64, 3, padding='same', activation='relu'),\n",
    "    MaxPooling2D(),\n",
    "    Conv2D(64, 3, padding='same', activation='relu'),\n",
    "    MaxPooling2D(),\n",
    "    Conv2D(32, 2, padding='same', activation='relu'),\n",
    "    MaxPooling2D(),\n",
    "    Flatten(),\n",
    "    Dense(256, activation='relu'),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "opt = SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "model.compile(optimizer=opt,\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's train and test our model. Keep in mind, the dataset is quite small so accuracy might be low. We will probably need to generate more augmented data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "4/4 [==============================] - 10s 2s/step - loss: 0.5009 - accuracy: 0.7857 - val_loss: 0.5268 - val_accuracy: 0.6667\n",
      "Epoch 2/30\n",
      "4/4 [==============================] - 11s 3s/step - loss: 0.4324 - accuracy: 0.8095 - val_loss: 0.5655 - val_accuracy: 0.7037\n",
      "Epoch 3/30\n",
      "4/4 [==============================] - 10s 2s/step - loss: 0.4342 - accuracy: 0.8175 - val_loss: 0.8250 - val_accuracy: 0.5556\n",
      "Epoch 4/30\n",
      "4/4 [==============================] - 11s 3s/step - loss: 0.4985 - accuracy: 0.7540 - val_loss: 0.5517 - val_accuracy: 0.6667\n",
      "Epoch 5/30\n",
      "4/4 [==============================] - 12s 3s/step - loss: 0.4055 - accuracy: 0.8254 - val_loss: 0.7417 - val_accuracy: 0.6296\n",
      "Epoch 6/30\n",
      "4/4 [==============================] - 14s 4s/step - loss: 0.5982 - accuracy: 0.6905 - val_loss: 0.5554 - val_accuracy: 0.7037\n",
      "Epoch 7/30\n",
      "4/4 [==============================] - 13s 3s/step - loss: 0.4516 - accuracy: 0.8016 - val_loss: 0.5565 - val_accuracy: 0.7407\n",
      "Epoch 8/30\n",
      "4/4 [==============================] - 13s 3s/step - loss: 0.4835 - accuracy: 0.8016 - val_loss: 0.5508 - val_accuracy: 0.7037\n",
      "Epoch 9/30\n",
      "4/4 [==============================] - 13s 3s/step - loss: 0.3929 - accuracy: 0.8333 - val_loss: 0.5805 - val_accuracy: 0.6667\n",
      "Epoch 10/30\n",
      "4/4 [==============================] - 14s 3s/step - loss: 0.3839 - accuracy: 0.8413 - val_loss: 0.5598 - val_accuracy: 0.7222\n",
      "Epoch 11/30\n",
      "4/4 [==============================] - 13s 3s/step - loss: 0.3717 - accuracy: 0.8492 - val_loss: 0.5521 - val_accuracy: 0.7407\n",
      "Epoch 12/30\n",
      "4/4 [==============================] - 14s 3s/step - loss: 0.6342 - accuracy: 0.7222 - val_loss: 0.5779 - val_accuracy: 0.7037\n",
      "Epoch 13/30\n",
      "4/4 [==============================] - 11s 3s/step - loss: 0.5210 - accuracy: 0.7857 - val_loss: 0.5119 - val_accuracy: 0.7593\n",
      "Epoch 14/30\n",
      "4/4 [==============================] - ETA: 0s - loss: 0.5105 - accuracy: 0.7381"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "model.fit(\n",
    "    train_generator,\n",
    "#     steps_per_epoch = train_generator.samples // BATCH_SIZE,\n",
    "    validation_data = validation_generator, \n",
    "#     validation_steps = validation_generator.samples // BATCH_SIZE,\n",
    "    epochs = 30,\n",
    "#     callbacks=[EarlyStopping(monitor='val_loss', patience=4)],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.88\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "print(model.history.history['val_accuracy'][-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our model is performing at about 88% cross-validated accuracy. Definitely not amazing, and there are architectural improvements that I have in mind to work on in the future, but this is far better than the 60% baseline I started out with. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
