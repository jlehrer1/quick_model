{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quickmodel\n",
    "A simple command project I'm putting together. Goal: User runs the file with a specific search term, and the program returns a pickled ML model that is trained to recognize images of the search term. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras \n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "img_dir = os.listdir('../data/')\n",
    "img_count = len(img_dir)\n",
    "type(img_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's generate our dataset from our existing files. Luckily, ``keras`` has a nice way of implementing this from their Image library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 117 images belonging to 2 classes.\n",
      "Found 50 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "image_generator = tf.keras.preprocessing.image.ImageDataGenerator(\n",
    "    rotation_range=20,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    horizontal_flip=True,\n",
    "    rescale=1./255, \n",
    "    validation_split=0.3\n",
    ")\n",
    "\n",
    "IMG_HEIGHT = 256\n",
    "IMG_WIDTH = 256\n",
    "BATCH_SIZE=20\n",
    "STEPS_PER_EPOCH = np.ceil(img_count/BATCH_SIZE)\n",
    "\n",
    "train_generator = image_generator.flow_from_directory(directory='../data',\n",
    "                                                     target_size=(IMG_HEIGHT, IMG_WIDTH),\n",
    "                                                     classes = ['Eggs', 'NOT-Eggs'], #should come from script parameter\n",
    "                                                     subset='training',\n",
    "                                                     class_mode='binary') \n",
    "validation_generator = image_generator.flow_from_directory(directory='../data',\n",
    "                                                          target_size=(IMG_HEIGHT, IMG_WIDTH),\n",
    "                                                          classes= ['Eggs', 'NOT-Eggs'],\n",
    "                                                          subset='validation',\n",
    "                                                          class_mode='binary')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have our dataset, we need to construct a model and train it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model = keras.models.Sequential([\n",
    "    Conv2D(32, 3, padding='same', activation='relu'),\n",
    "    MaxPooling2D(),\n",
    "    Conv2D(64, 3, padding='same', activation='relu'),\n",
    "    MaxPooling2D(),\n",
    "    Conv2D(128, 3, padding='same', activation='relu'),\n",
    "    MaxPooling2D(),\n",
    "    Conv2D(32, 2, padding='same', activation='relu'),\n",
    "    MaxPooling2D(),\n",
    "    Flatten(),\n",
    "    Dense(512, activation='relu'),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "opt = SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "model.compile(optimizer=opt,\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's train and test our model. Keep in mind, the dataset is quite small so accuracy might be low. We will probably need to generate more augmented data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "5/5 [==============================] - 20s 4s/step - loss: 0.5883 - accuracy: 0.6779 - val_loss: 0.5696 - val_accuracy: 0.6400\n",
      "Epoch 2/100\n",
      "5/5 [==============================] - 17s 3s/step - loss: 0.5414 - accuracy: 0.7517 - val_loss: 0.5544 - val_accuracy: 0.6600\n",
      "Epoch 3/100\n",
      "5/5 [==============================] - 16s 3s/step - loss: 0.5461 - accuracy: 0.7246 - val_loss: 0.5481 - val_accuracy: 0.7200\n",
      "Epoch 4/100\n",
      "5/5 [==============================] - 19s 4s/step - loss: 0.4513 - accuracy: 0.8188 - val_loss: 0.5408 - val_accuracy: 0.6800\n",
      "Epoch 5/100\n",
      "5/5 [==============================] - 17s 3s/step - loss: 0.5092 - accuracy: 0.7785 - val_loss: 0.4151 - val_accuracy: 0.8400\n",
      "Epoch 6/100\n",
      "5/5 [==============================] - 18s 4s/step - loss: 0.4571 - accuracy: 0.8054 - val_loss: 0.6932 - val_accuracy: 0.6200\n",
      "Epoch 7/100\n",
      "5/5 [==============================] - 18s 4s/step - loss: 0.6508 - accuracy: 0.6711 - val_loss: 0.5291 - val_accuracy: 0.7400\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x63406b850>"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "model.fit_generator(\n",
    "    train_generator,\n",
    "    steps_per_epoch = train_generator.samples // BATCH_SIZE,\n",
    "    validation_data = validation_generator, \n",
    "    validation_steps = validation_generator.samples // BATCH_SIZE,\n",
    "    epochs = 30,\n",
    "    callbacks=[EarlyStopping(monitor='val_loss', patience=4)],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': [0.5906243764314075, 0.5354454469360761, 0.5483702971883442, 0.4510720722227289, 0.5034363777845497, 0.4568101643715929, 0.6378331368401546], 'accuracy': [0.67785233, 0.7516779, 0.7246377, 0.8187919, 0.7785235, 0.80536914, 0.67114097], 'val_loss': [0.5696409642696381, 0.5544193685054779, 0.5481173247098923, 0.5408102869987488, 0.415128618478775, 0.6931792795658112, 0.529105007648468], 'val_accuracy': [0.64, 0.66, 0.72, 0.68, 0.84, 0.62, 0.74]}\n"
     ]
    }
   ],
   "source": [
    "print(model.history.history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
