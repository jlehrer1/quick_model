{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quickmodel\n",
    "A simple command project I'm putting together. Goal: User runs the file with a specific search term, and the program returns a pickled ML model that is trained to recognize images of the search term. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras \n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "img_dir = os.listdir('../data/')\n",
    "img_count = len(img_dir)\n",
    "type(img_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's generate our dataset from our existing files. Luckily, ``keras`` has a nice way of implementing this from their Image library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 117 images belonging to 2 classes.\n",
      "Found 50 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "image_generator = tf.keras.preprocessing.image.ImageDataGenerator(\n",
    "    rotation_range=20,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    horizontal_flip=True,\n",
    "    rescale=1./255, \n",
    "    validation_split=0.3\n",
    ")\n",
    "\n",
    "IMG_HEIGHT = 256\n",
    "IMG_WIDTH = 256\n",
    "BATCH_SIZE=20\n",
    "STEPS_PER_EPOCH = np.ceil(img_count/BATCH_SIZE)\n",
    "\n",
    "train_generator = image_generator.flow_from_directory(directory='../data',\n",
    "                                                     target_size=(IMG_HEIGHT, IMG_WIDTH),\n",
    "                                                     classes = ['Eggs', 'NOT-Eggs'], #should come from script parameter\n",
    "                                                     subset='training',\n",
    "                                                     class_mode='binary') \n",
    "validation_generator = image_generator.flow_from_directory(directory='../data',\n",
    "                                                          target_size=(IMG_HEIGHT, IMG_WIDTH),\n",
    "                                                          classes= ['Eggs', 'NOT-Eggs'],\n",
    "                                                          subset='validation',\n",
    "                                                          class_mode='binary')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have our dataset, we need to construct a model and train it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.Sequential([\n",
    "    Conv2D(32, 3, padding='same', activation='relu'),\n",
    "    MaxPooling2D(),\n",
    "    Conv2D(64, 3, padding='same', activation='relu'),\n",
    "    MaxPooling2D(),\n",
    "    Conv2D(128, 3, padding='same', activation='relu'),\n",
    "    MaxPooling2D(),\n",
    "    Conv2D(32, 2, padding='same', activation='relu'),\n",
    "    MaxPooling2D(),\n",
    "    Flatten(),\n",
    "    Dense(512, activation='relu'),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "opt = SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "model.compile(optimizer=opt,\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's train and test our model. Keep in mind, the dataset is quite small so accuracy might be low. We will probably need to generate more augmented data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "5/5 [==============================] - 20s 4s/step - loss: 0.6916 - accuracy: 0.5298 - val_loss: 0.6858 - val_accuracy: 0.6000\n",
      "Epoch 2/30\n",
      "5/5 [==============================] - 31s 6s/step - loss: 0.6776 - accuracy: 0.5815 - val_loss: 0.6763 - val_accuracy: 0.6000\n",
      "Epoch 3/30\n",
      "5/5 [==============================] - 19s 4s/step - loss: 0.6764 - accuracy: 0.5451 - val_loss: 0.6538 - val_accuracy: 0.6600\n",
      "Epoch 4/30\n",
      "5/5 [==============================] - 20s 4s/step - loss: 0.6424 - accuracy: 0.6435 - val_loss: 0.6364 - val_accuracy: 0.5800\n",
      "Epoch 5/30\n",
      "5/5 [==============================] - 19s 4s/step - loss: 0.5981 - accuracy: 0.6064 - val_loss: 0.5947 - val_accuracy: 0.7200\n",
      "Epoch 6/30\n",
      "5/5 [==============================] - 21s 4s/step - loss: 0.6414 - accuracy: 0.6934 - val_loss: 0.5906 - val_accuracy: 0.7800\n",
      "Epoch 7/30\n",
      "5/5 [==============================] - 21s 4s/step - loss: 0.5713 - accuracy: 0.7216 - val_loss: 0.5742 - val_accuracy: 0.7000\n",
      "Epoch 8/30\n",
      "5/5 [==============================] - 20s 4s/step - loss: 0.5352 - accuracy: 0.7132 - val_loss: 0.6390 - val_accuracy: 0.6400\n",
      "Epoch 9/30\n",
      "5/5 [==============================] - 18s 4s/step - loss: 0.5059 - accuracy: 0.7806 - val_loss: 0.4749 - val_accuracy: 0.7400\n",
      "Epoch 10/30\n",
      "5/5 [==============================] - 20s 4s/step - loss: 0.5324 - accuracy: 0.7271 - val_loss: 0.5358 - val_accuracy: 0.7400\n",
      "Epoch 11/30\n",
      "5/5 [==============================] - 22s 4s/step - loss: 0.5763 - accuracy: 0.7146 - val_loss: 0.5061 - val_accuracy: 0.7200\n",
      "Epoch 12/30\n",
      "5/5 [==============================] - 23s 5s/step - loss: 0.4993 - accuracy: 0.7560 - val_loss: 0.4531 - val_accuracy: 0.7400\n",
      "Epoch 13/30\n",
      "5/5 [==============================] - 23s 5s/step - loss: 0.5156 - accuracy: 0.6741 - val_loss: 0.4653 - val_accuracy: 0.8000\n",
      "Epoch 14/30\n",
      "5/5 [==============================] - 20s 4s/step - loss: 0.4127 - accuracy: 0.8009 - val_loss: 0.4308 - val_accuracy: 0.7800\n",
      "Epoch 15/30\n",
      "5/5 [==============================] - 21s 4s/step - loss: 0.3809 - accuracy: 0.8027 - val_loss: 0.5319 - val_accuracy: 0.7800\n",
      "Epoch 16/30\n",
      "5/5 [==============================] - 21s 4s/step - loss: 0.4437 - accuracy: 0.7779 - val_loss: 0.4278 - val_accuracy: 0.7400\n",
      "Epoch 17/30\n",
      "5/5 [==============================] - 19s 4s/step - loss: 0.3436 - accuracy: 0.8801 - val_loss: 0.4236 - val_accuracy: 0.8400\n",
      "Epoch 18/30\n",
      "5/5 [==============================] - 17s 3s/step - loss: 0.3380 - accuracy: 0.8349 - val_loss: 0.3288 - val_accuracy: 0.8000\n",
      "Epoch 19/30\n",
      "5/5 [==============================] - 19s 4s/step - loss: 0.5015 - accuracy: 0.8264 - val_loss: 0.4201 - val_accuracy: 0.8400\n",
      "Epoch 20/30\n",
      "5/5 [==============================] - 20s 4s/step - loss: 0.4331 - accuracy: 0.7698 - val_loss: 0.4350 - val_accuracy: 0.8000\n",
      "Epoch 21/30\n",
      "5/5 [==============================] - 19s 4s/step - loss: 0.3529 - accuracy: 0.8238 - val_loss: 0.4072 - val_accuracy: 0.8600\n",
      "Epoch 22/30\n",
      "5/5 [==============================] - 19s 4s/step - loss: 0.4201 - accuracy: 0.8325 - val_loss: 0.4550 - val_accuracy: 0.8200\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x635b8d8d0>"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "model.fit_generator(\n",
    "    train_generator,\n",
    "    steps_per_epoch = train_generator.samples // BATCH_SIZE,\n",
    "    validation_data = validation_generator, \n",
    "    validation_steps = validation_generator.samples // BATCH_SIZE,\n",
    "    epochs = 30,\n",
    "    callbacks=[EarlyStopping(monitor='val_loss', patience=4)],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.74\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "print(np.mean(model.history.history['val_accuracy']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our model is performing at about 82% accuracy. Definitely not amazing, and there are architectural improvements that I have in mind to work on in the future, but this is far better than the 60% baseline I started out with. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
